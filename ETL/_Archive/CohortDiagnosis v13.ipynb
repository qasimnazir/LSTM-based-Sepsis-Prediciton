{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"CohortDiagnosis v13.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"XTDu0r5Dl4UU"},"source":["import pyspark\n","import time\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","#from pyspark.sql.functions import sequence, to_date, explode, col\n","from pyspark.sql.types import *\n","from pyspark.sql.window import Window\n","\n","spark = SparkSession.builder.getOrCreate() "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uvYM2ZFSl4Ud"},"source":["#Hardcode schema to speed up data read\n","#Code Reference:https://github.com/MIT-LCP/mimic-code/blob/master/buildmimic/aws-athena/mimictoparquet_glue_job.py\n","schema_icustays = StructType([\n","    StructField(\"row_id\", IntegerType()),\n","    StructField(\"subject_id\", IntegerType()),\n","    StructField(\"hadm_id\", IntegerType()),\n","    StructField(\"icustay_id\", IntegerType()),\n","    StructField(\"dbsource\", StringType()),\n","    StructField(\"first_careunit\", StringType()),\n","    StructField(\"last_careunit\", StringType()),\n","    StructField(\"first_wardid\", ShortType()),\n","    StructField(\"last_wardid\", ShortType()),\n","    StructField(\"intime\", TimestampType()),\n","    StructField(\"outtime\", TimestampType()),\n","    StructField(\"los\", DoubleType())\n","])\n","\n","schema_patients = StructType([\n","    StructField(\"row_id\", IntegerType()),\n","    StructField(\"subject_id\", IntegerType()),\n","    StructField(\"gender\", StringType()),\n","    StructField(\"dob\", TimestampType()),\n","    StructField(\"dod\", TimestampType()),\n","    StructField(\"dod_hosp\", TimestampType()),\n","    StructField(\"dod_ssn\", TimestampType()),\n","    StructField(\"expire_flag\", IntegerType())\n","])\n","\n","schema_services = StructType([\n","    StructField(\"row_id\", IntegerType()),\n","    StructField(\"subject_id\", IntegerType()),\n","    StructField(\"hadm_id\", IntegerType()),\n","    StructField(\"transfertime\", TimestampType()),\n","    StructField(\"prev_service\", StringType()),\n","    StructField(\"curr_service\", StringType())\n","])\n","\n","schema_chartevents = StructType([\n","    StructField(\"row_id\", IntegerType()),\n","    StructField(\"subject_id\", IntegerType()),\n","    StructField(\"hadm_id\", IntegerType()),\n","    StructField(\"icustay_id\", IntegerType()),\n","    StructField(\"itemid\", IntegerType()),\n","    StructField(\"charttime\", TimestampType()),\n","    StructField(\"storetime\", TimestampType()),\n","    StructField(\"cgid\", IntegerType()),\n","    StructField(\"value\", StringType()),\n","    StructField(\"valuenum\", DoubleType()),\n","    StructField(\"valueuom\", StringType()),\n","    StructField(\"warning\", IntegerType()),\n","    StructField(\"error\", IntegerType()),\n","    StructField(\"resultstatus\", StringType()),\n","    StructField(\"stopped\", StringType())\n","])\n","\n","\n","schema_ditems = StructType([\n","    StructField(\"row_id\", IntegerType()),\n","    StructField(\"itemid\", IntegerType()),\n","    StructField(\"label\", StringType()),\n","    StructField(\"abbreviation\", StringType()),\n","    StructField(\"dbsource\", StringType()),\n","    StructField(\"linksto\", StringType()),\n","    StructField(\"category\", StringType()),\n","    StructField(\"unitname\", StringType()),\n","    StructField(\"param_type\", StringType()),\n","    StructField(\"conceptid\", IntegerType())\n","])\n","\n","schema_admissions = StructType([\n","    StructField(\"row_id\", IntegerType()),\n","    StructField(\"subject_id\", IntegerType()),\n","    StructField(\"hadm_id\", IntegerType()),\n","    StructField(\"admittime\", TimestampType()),\n","    StructField(\"dischtime\", TimestampType()),\n","    StructField(\"deathtime\", TimestampType()),\n","    StructField(\"admission_type\", StringType()),\n","    StructField(\"admission_location\", StringType()),\n","    StructField(\"discharge_location\", StringType()),\n","    StructField(\"insurance\", StringType()),\n","    StructField(\"language\", StringType()),\n","    StructField(\"religion\", StringType()),\n","    StructField(\"marital_status\", StringType()),\n","    StructField(\"ethnicity\", StringType()),\n","    StructField(\"edregtime\", TimestampType()),\n","    StructField(\"edouttime\", TimestampType()),\n","    StructField(\"diagnosis\", StringType()),\n","    StructField(\"hospital_expire_flag\", ShortType()),\n","    StructField(\"has_chartevents_data\", ShortType())\n","])\n","\n","\n","schema_fio2 = StructType([\n","    StructField(\"icustay_id\", IntegerType()),\n","    StructField(\"charttime\", TimestampType()),\n","    StructField(\"fio2\", DoubleType())\n","    \n","])\n","\n","schema_gcs = StructType([\n","    StructField(\"icustay_id\", IntegerType()),\n","    StructField(\"charttime\", TimestampType()),\n","    StructField(\"gcs\", DoubleType()),\n","    StructField(\"gcsmotor\", DoubleType()),\n","    StructField(\"gcsverbal\", DoubleType()),\n","    StructField(\"gcseyes\", DoubleType()),\n","    StructField(\"endotrachflag\", IntegerType())    \n","])\n","\n","schema_sofa = StructType([\n","    StructField(\"icustay_id\", IntegerType()),\n","    StructField(\"hr\", IntegerType()),\n","    StructField(\"starttime\", TimestampType()),\n","    StructField(\"endtime\", TimestampType()),\n","    StructField(\"sofa_24hours\", IntegerType())\n","])\n","\n","\n","\n","\n","schema_vital = StructType([\n","    StructField(\"icustay_id\", IntegerType()),\n","    StructField(\"charttime\", TimestampType()),\n","    StructField(\"heartrate\", DoubleType()),\n","    StructField(\"sysbp\", DoubleType()),\n","    StructField(\"diasbp\", DoubleType()),\n","    StructField(\"meanbp\", DoubleType()),\n","    StructField(\"resprate\", DoubleType()),\n","    StructField(\"tempc\", DoubleType()),\n","    StructField(\"spo2\", DoubleType()),\n","    StructField(\"glucose\", DoubleType())\n","])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FVsgCjT4l4Ug"},"source":["#start_time = time.time()\n","df_icustays = spark.read.csv('gs://peaceful-bruin-307600/db/ICUSTAYS.csv', sep = ',', schema = schema_icustays, header = True)\n","df_patients = spark.read.csv('gs://peaceful-bruin-307600/db/PATIENTS.csv', sep = ',', schema = schema_patients, header = True)\n","df_services = spark.read.csv('gs://peaceful-bruin-307600/db/SERVICES.csv', sep = ',', schema = schema_services, header = True)\n","df_chartevents = spark.read.csv('gs://peaceful-bruin-307600/db/CHARTEVENTS.csv', sep = ',', schema = schema_chartevents, header = True)\n","df_admissions = spark.read.csv('gs://peaceful-bruin-307600/db/ADMISSIONS.csv', sep = ',', schema = schema_admissions, header = True)\n","df_ditems = spark.read.csv('gs://peaceful-bruin-307600/db/D_ITEMS.csv', sep = ',', schema = schema_ditems, header = True)\n","df_fio2 = spark.read.csv('gs://peaceful-bruin-307600/derived/fio2.csv', sep = ',', schema = schema_fio2, header = True)\n","df_gcs = spark.read.csv('gs://peaceful-bruin-307600/derived/gcs.csv', sep = ',', schema = schema_gcs, header = True)\n","df_sofa = spark.read.csv('gs://peaceful-bruin-307600/derived/sofa_direct', sep = ',', schema = schema_sofa, header = True)\n","df_vital = spark.read.csv('gs://peaceful-bruin-307600/derived/vital.csv', sep = ',', schema = schema_vital, header = True)\n","df_sepsis_no_exclusion = spark.read.csv('gs://peaceful-bruin-307600/sepsis3-df-no-exclusions.csv', sep = ',', inferSchema= True, header = True)\n","\n","#end_time = time.time()\n","#print(\"Total execution time: {} seconds\".format(end_time - start_time))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NslEIfj_l4Uh"},"source":["#Create Temporary Tables for query\n","df_icustays.registerTempTable('icustays')\n","df_patients.registerTempTable('patients')\n","df_services.registerTempTable('services')\n","df_chartevents.registerTempTable('chartevents')\n","df_admissions.registerTempTable('admissions')\n","df_ditems.registerTempTable('ditems')\n","df_fio2.registerTempTable('fio2')\n","df_gcs.registerTempTable('gcs')\n","df_sofa.registerTempTable('sofa')\n","df_vital.registerTempTable('vital')\n","df_sepsis_no_exclusion.registerTempTable('sepsis3')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jr5KW_Ajl4Ui"},"source":["#Get Suspected infection time for each icustay_id\n","query = \\\n","\"\"\"\n","select\n","    Distinct\n","    icustay_id\n","    ,suspected_infection_time_poe\n","    ,suspected_infection_time_poe_days\n","\n","from sepsis3\n","    Where suspected_infection_time_poe is NOT NULL\n","\n","\"\"\"\n","\n","df_susp_inf = spark.sql(query)\n","df_susp_inf.registerTempTable('susp_inf')\n","#df_susp_inf.count()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vOr10y-6l4Ui"},"source":["#Code Reference: https://github.com/MIT-LCP/mimic-code/blob/7ff270c7079a42621f6e011de6ce4ddc0f7fd45c/tutorials/cohort-selection.ipynb\n","#Code Reference: https://github.com/alistairewj/sepsis3-mimic/blob/master/query/tbls/cohort.sql\n","query = \\\n","\"\"\"\n","WITH co AS\n","(\n","SELECT \n","icu.subject_id\n",",icu.hadm_id\n",",icu.icustay_id\n",",icu.dbsource\n",",first_careunit\n",",los as icu_length_of_stay\n",",icu.intime\n",",icu.outtime\n",",DATEDIFF (icu.intime , pat.dob )/365 as age\n",",pat.gender\n",",adm.ethnicity\n",",adm.HAS_CHARTEVENTS_DATA\n",",RANK() OVER (PARTITION BY icu.subject_id ORDER BY icu.intime) AS icustay_id_order\n","FROM icustays icu\n","INNER JOIN patients pat ON icu.subject_id = pat.subject_id\n","INNER JOIN admissions adm ON icu.hadm_id = adm.hadm_id\n","--LIMIT 10\n",")\n",",serv AS\n","(\n","SELECT \n","icu.*\n",",se.curr_service\n",",CASE\n","--WHEN curr_service like '%SURG' then 1\n","--WHEN curr_service = 'ORTHO' then 1\n","WHEN curr_service in ('CSURG','VSURG','TSURG') then 1\n","ELSE 0 END\n","as surgical\n",",RANK() OVER (PARTITION BY icu.hadm_id ORDER BY se.transfertime DESC) as rank\n","FROM icustays icu\n","LEFT JOIN services se ON icu.hadm_id = se.hadm_id\n","--AND se.transfertime < icu.intime + interval '12' hour\n",")\n","\n","SELECT\n","co.*\n",",CASE\n","WHEN co.icu_length_of_stay < .5 then 1\n","ELSE 0 END\n","AS exclusion_los\n",",CASE\n","WHEN co.age <= 16 then 1\n","ELSE 0 END\n","AS exclusion_age\n",",CASE \n","WHEN co.icustay_id_order != 1 THEN 1\n","ELSE 0 END \n","AS exclusion_first_stay\n",",CASE\n","WHEN serv.surgical == 1 THEN 1\n","ELSE 0 END\n","as exclusion_surgical\n",",CASE\n","when co.dbsource != 'metavision' THEN 1\n","ELSE 0 END \n","as exclusion_icu_db\n",",Case \n","when co.HAS_CHARTEVENTS_DATA == 0 then 1\n","when co.intime is null then 1\n","when co.outtime is null then 1\n","else 0 end \n","as exclusion_bad_data\n","\n",",inf.suspected_infection_time_poe\n",",inf.suspected_infection_time_poe - INTERVAL 48 HOURS as inf_window_start\n",",inf.suspected_infection_time_poe + INTERVAL 24 HOURS as inf_window_end\n","\n","--Exclude cases where there is no overlap with suspected infection window\n",",Case \n","--The line below limits suspected infection that occur outside the end of the ICU Stay to ensure at least 12 hours of data are avaliable for SOFA score window\n","when (inf.suspected_infection_time_poe - INTERVAL 48 HOURS) > (co.outtime - INTERVAL 12 HOURS) Then 1\n","--The line below limits suspected infection time to within 12 hours prior to ICU stay to ensure enough data points for sofa window \n","--Cuts out Noise for specificity as suspected infection time is often round to day and can lead to misleading no sepsis diagnosis ground truth\n","when (inf.suspected_infection_time_poe + INTERVAL 24 HOURS ) < (co.intime + INTERVAL 12 HOURS) Then 1\n","else 0 end \n","as exclusion_sus_inf_window\n","\n","--Exclude cases where suspected infection time is outside of icu stay\n","--,Case \n","--when (inf.suspected_infection_time_poe) > co.outtime then 1\n","--when (inf.suspected_infection_time_poe) < co.intime then 1\n","--Keep patients with no suspected infection time\n","--when inf.suspected_infection_time_poe IS NULL then 0\n","--else 0 end \n","--as exclusion_sus_inf_window\n","\n","--Exclude cases where suspected infection time is outside of icu stay\n","--,Case \n","--Earlier than one day of ICU Stay\n","--when suspected_infection_time_poe_days < -1 then 1\n","--Later than ICU Stay\n","--when (inf.suspected_infection_time_poe) > co.outtime then 1\n","--else 0 end \n","--as exclusion_sus_inf_window\n","\n","--Exclude cases where suspected infection time is outside of icu stay\n","--,Case \n","--ensure sepsis diagnosis window is within icu window\n","--when (inf.suspected_infection_time_poe - INTERVAL 48 HOURS)  < co.intime then 1\n","--when (inf.suspected_infection_time_poe + INTERVAL 24 HOURS) > co.outtime then 1\n","--includes patients without susp. infection time\n","--else 0 end \n","--as exclusion_sus_inf_window\n","\n","\n","FROM co\n","LEFT JOIN serv ON co.icustay_id = serv.icustay_id AND serv.rank = 1\n","LEFT JOIN susp_inf inf ON (co.icustay_id = inf.icustay_id)\n","\n","\"\"\"    \n","df_cohort_no_exclusion = spark.sql(query)\n","#df_cohort_no_exclusion.count()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a7d-2S1Hl4Uk","outputId":"c0e7c850-1b2f-4721-d100-b0cf8a6977e1"},"source":["#Filter Cohort for Exclusions\n","df_cohort_exclusion = df_cohort_no_exclusion.filter((df_cohort_no_exclusion.exclusion_age == 0)\n","                                                    & (df_cohort_no_exclusion.exclusion_first_stay == 0)\n","                                                    & (df_cohort_no_exclusion.exclusion_surgical == 0) \n","                                                    & (df_cohort_no_exclusion.exclusion_icu_db == 0)\n","                                                    & (df_cohort_no_exclusion.exclusion_bad_data == 0)\n","                                                    & (df_cohort_no_exclusion.exclusion_sus_inf_window == 0)\n","                                                    & (df_cohort_no_exclusion.exclusion_los == 0)) \n","\n","df_cohort_exclusion.registerTempTable('cohort_exclusion')\n","df_cohort_exclusion.count()\n","#df_cohort_exclusion.repartition(1).write.option(\"header\", \"true\").csv('gs://peaceful-bruin-307600/cohort_nonts_v2.csv')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8569"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"BqLbo0XLl4Ul"},"source":["#Create TS with an additional field of array time sequences by hour between the intime and outtime for a subjet_id, icustay_id\n","#Code Reference: https://stackoverflow.com/questions/43141671/sparksql-on-pyspark-how-to-generate-time-series\n","query = \\\n","\"\"\"\n","SELECT\n","--Distinct\n","ce.*\n",",DATE_TRUNC('hour', ce.intime) as intime_round\n",",DATE_TRUNC('hour', ce.outtime)+ INTERVAL 1 HOURS as outtime_round\n","--,sequence(to_timestamp(DATE_TRUNC('hour', ce.intime)), to_timestamp(DATE_TRUNC('hour', ce.outtime) + INTERVAL 1 HOURS), interval 1 hour) as time\n",",sequence(to_timestamp(DATE_TRUNC('hour', ce.intime)), to_timestamp(DATE_TRUNC('hour', ce.outtime)), interval 1 hour) as time\n","\n","FROM cohort_exclusion ce\n","\"\"\"\n","\n","df_cohort_exclusion_ts = sqlContext.sql(query)\n","#df_cohort_exclusion_ts.count()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tCTg0TGJl4Um"},"source":["#Explode the array field for each row into multiple rows to create a time series template\n","#df_cohort_exclusion_ts = df_cohort_exclusion_ts.withColumn(\"timestamp\", explode(col(\"time\"))).drop(col(\"time\"))\n","df_cohort_exclusion_ts = df_cohort_exclusion_ts.select(\"*\", posexplode(col(\"time\"))).drop(col(\"time\"))\n","df_cohort_exclusion_ts = df_cohort_exclusion_ts.withColumnRenamed('col', 'timestamp')\n","df_cohort_exclusion_ts = df_cohort_exclusion_ts.withColumnRenamed('pos', 'hour')\n","\n","#Register Table for querying\n","df_cohort_exclusion_ts.registerTempTable('cohort_exclusion_ts')\n","#df_cohort_exclusion_ts.repartition(1).write.option(\"header\", \"true\").csv('gs://peaceful-bruin-307600/cohort_test_case_pos4.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aa5s8tA6l4Um"},"source":["#Cleanse sofa prior to joining\n","query = \\\n","\"\"\"\n","select \n","    s.icustay_id\n","    ,s.hr\n","    ,s.starttime\n","    ,s.endtime\n","    ,s.sofa_24hours\n","    \n","from sofa s \n","where 1=1\n","\n","\"\"\"    \n","\n","df_sofa_cleansed = sqlContext.sql(query)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ul9T-EDl4Um"},"source":["#Impute Last and First value for missing sofa 24 hour scores\n","\n","#Fill in Missing Values with Last Value if avaliable followed by latest value for missing preonset data\n","#Code Reference: Paul Lee's Lab Notebook, https://stackoverflow.com/questions/38131982/forward-fill-missing-values-in-spark-python\n","window = Window.partitionBy('icustay_id')\\\n","       .orderBy('hr')\\\n","       .rowsBetween(-1000000, 0)\n","\n","#colsfill = ['v_heartrate', 'v_sysbp', 'v_diasbp', 'v_meanbp', 'v_resprate', 'v_tempc', 'v_spo2', 'v_glucose']\n","colsfill = ['sofa_24hours']\n","            \n","for col in colsfill:\n","    df_sofa_cleansed = df_sofa_cleansed.withColumn(col, last(col,ignorenulls = True).over(window))   \n","\n","window = Window.partitionBy('icustay_id')\\\n","       .orderBy('hr')\\\n","       .rowsBetween(0, Window.unboundedFollowing)\n","\n","for col in colsfill:\n","    df_sofa_cleansed =df_sofa_cleansed.withColumn(col, first(col,ignorenulls = True).over(window))   \n","    \n","\n","df_sofa_cleansed.registerTempTable('sofa_cleansed')\n","#df_sofa_test.repartition(1).write.option(\"header\", \"true\").csv('gs://peaceful-bruin-307600/sofa_test.csv')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EI4kVFygl4Un"},"source":["#JOIN SOFA to cohort ts\n","\n","query = \\\n","\"\"\"\n","select \n","    ts.*\n","    --,s.hr\n","    --,s.starttime\n","    --,s.endtime\n","    ,s.sofa_24hours\n","    \n","from cohort_exclusion_ts ts \n","        LEFT JOIN sofa_cleansed s ON (ts.icustay_id = s.icustay_id) AND (ts.timestamp = s.starttime)\n"," \n","where 1=1\n","\"\"\"    \n","\n","df_cohort_cleansed = sqlContext.sql(query)\n","df_cohort_cleansed.registerTempTable('cohort_cleansed')\n","#df_cohort.show()\n","#df_cohort.repartition(1).write.option(\"header\", \"true\").csv('gs://peaceful-bruin-307600/cohort_cleansed.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ywM86mcKl4Un"},"source":["#DETERMINES DIAGNOSIS FOR Sepsis based on SOFA >= 2 over 48 hours prior and 24 hours after suspected infection time limited by icu stay time window. Also seeks increase based on first sofa 24 hour value in time window.\n","query = \\\n","\"\"\"\n","With Inf_W_CTE AS (\n","--Flags Window over Time Series for suspected infection: 48 hrs preceding and 24 post suspicion\n","Select \n","    cc.*\n","    ,CASE WHEN timestamp BETWEEN cc.inf_window_start AND cc.inf_window_end - INTERVAL 1 HOURS Then 1\n","          ELSE 0 \n","          END AS sus_window_flg\n","    \n","from cohort_cleansed as cc\n",")\n","\n",",DIAG_CTE as (\n","--Compares sofa_24hours score with min. score prior to current row and flags when a geq +2 change occurs \n","Select\n","    W.icustay_id\n","    ,W.hour\n","    ,W.sus_window_flg\n","    ,sofa_24hours - (Min(sofa_24hours) OVER(PARTITION BY icustay_id ORDER BY hour ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING)) as sofa_24hours_delta\n","    --,sofa_24hours - (FIRST_VALUE(sofa_24hours) OVER(PARTITION BY icustay_id ORDER BY hour)) as sofa_24hours_delta\n","    ,CASE WHEN (sofa_24hours - (Min(sofa_24hours) OVER(PARTITION BY icustay_id ORDER BY hour ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING))) >= 2 Then 1\n","          ELSE 0\n","          END AS Sepsis3_start_flg\n","          \n","From INF_W_CTE as W\n","Where W.sus_window_flg = 1\n",")\n","\n","Select \n","    cc.*\n","    --,d.sus_window_flg\n","    --,d.sofa_24hours_delta\n","    ,d.Sepsis3_start_flg\n","    ,CASE WHEN (SUM(d.Sepsis3_start_flg) OVER (PARTITION BY cc.icustay_id) >= 1) THEN 1 ELSE 0 END as Sepsis3_diag_flg \n","FROM cohort_cleansed as cc\n","LEFT JOIN DIAG_CTE as d ON (cc.icustay_id = d.icustay_id) AND (cc.hour = d.hour)\n","\"\"\"  \n","\n","df_cohort_diag = sqlContext.sql(query)\n","df_cohort_diag.registerTempTable('cohort_diag')\n","#df_cohort_diag.repartition(1).write.option(\"header\", \"true\").csv('gs://peaceful-bruin-307600/cohort_diag_v10.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iMon4hGGl4Un"},"source":["#Find Sespsi Onset Hour\n","\n","query = \\\n","\"\"\"\n","With first_cte as (\n","\n","    Select\n","        cd.icustay_id\n","        ,cd.hour\n","        ,ROW_NUMBER() OVER(PARTITION BY cd.icustay_id order by cd.hour) as rn\n","\n","    from cohort_diag as cd\n","    Where Sepsis3_start_flg = 1\n",")\n","\n","select\n","cd.*\n",",fc.hour as sepsis_onset_hr\n","from cohort_diag cd left join first_cte fc ON (cd.icustay_id = fc.icustay_id) AND (cd.hour = fc.hour) AND fc.rn = 1 \n","--where cd.icustay_id = 234778\n","\n","\n","\"\"\"  \n","df_cohort_diag_onset = sqlContext.sql(query)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V0HiFa12l4Uo"},"source":["#Impute Last and First value for sepsis onset hour\n","\n","#Fill in Missing Values with Last Value if avaliable followed by latest value for missing preonset data\n","#Code Reference: Paul Lee's Lab Notebook, https://stackoverflow.com/questions/38131982/forward-fill-missing-values-in-spark-python\n","window = Window.partitionBy('icustay_id')\\\n","       .orderBy('hour')\\\n","       .rowsBetween(-1000000, 0)\n","\n","colsfill = ['sepsis_onset_hr']\n","            \n","for col in colsfill:\n","    df_cohort_diag_onset = df_cohort_diag_onset.withColumn(col, last(col,ignorenulls = True).over(window))   \n","\n","window = Window.partitionBy('icustay_id')\\\n","       .orderBy('hour')\\\n","       .rowsBetween(0, Window.unboundedFollowing)\n","\n","for col in colsfill:\n","    df_cohort_diag_onset =df_cohort_diag_onset.withColumn(col, first(col,ignorenulls = True).over(window))   \n","    \n","df_cohort_diag_onset.registerTempTable('cohort_diag_onset')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I1k_yZ7-l4Uo"},"source":["#Determines obs and prediction windows\n","query = \\\n","\"\"\"\n","with control_cte as (\n","    select icustay_id\n","           --,round(((icu_length_of_stay*24)/4),0) as control_index_hr\n","           ,max(hour) as control_index_hr\n","           ,(max(hour) - 12) as control_start_hr\n","    from cohort_diag_onset \n","    where sepsis_onset_hr is NULL\n","    group by icustay_id\n",")\n","\n","select cdo.* \n","from cohort_diag_onset as cdo \n","left join control_cte as cc on cdo.icustay_id = cc.icustay_id\n","\n","where 1=1\n","      --Sepsis window is prior to onset with min of six data points, control is up to last 12 observation of patient stay\n","      and (((hour < sepsis_onset_hr) and (sepsis_onset_hr > 6)) \n","             or ((sepsis_onset_hr is NULL) and (hour <= control_index_hr) and (hour > control_start_hr))) \n","\"\"\"\n","\n","df_cohort_diag_onset_final = sqlContext.sql(query)\n","df_cohort_diag_onset_final.repartition(1).write.option(\"header\", \"true\").csv('gs://peaceful-bruin-307600/cohort_diag_v15.csv')\n","#df_cohort_diag_onset_final.registerTempTable('cohort_diag_onset_final')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mD-rfQFil4Uo"},"source":["#Test Diagnosis\n","query = \\\n","\"\"\"\n","select\n","icustay_id\n",",Sepsis3_diag_flg\n",",count(*) as cntchk\n",",sum(Sepsis3_diag_flg) as sumchk\n","\n","from cohort_diag cd\n","\n","group by icustay_id, Sepsis3_diag_flg\n","\"\"\"  \n","\n","df_cohort_diag_test = sqlContext.sql(query)\n","df_cohort_diag_test.repartition(1).write.option(\"header\", \"true\").csv('gs://peaceful-bruin-307600/cohort_diag_test.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4P2dDmOgl4Uo"},"source":["#Test Diagnosis\n","query = \\\n","\"\"\"\n","select *  \n","from cohort_diag_onset_final\n","where icustay_id in (298259)\n","\"\"\"  \n","\n","df_cohort_diag_test2 = sqlContext.sql(query)\n","#df_cohort_diag_test2.head()\n","df_cohort_diag_test2.repartition(1).write.option(\"header\", \"true\").csv('gs://peaceful-bruin-307600/test_298259 v6.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ho5SAIkgl4Up"},"source":["#Test Diagnosis\n","query = \\\n","\"\"\"\n","select cd.*  \n","from cohort_diag cd\n","where icustay_id in (200033)\n","\"\"\"  \n","\n","df_cohort_diag_test2 = sqlContext.sql(query)\n","#df_cohort_diag_test2.repartition(1).write.option(\"header\", \"true\").csv('gs://peaceful-bruin-307600/cohort_diag_test3.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bNOZb5Apl4Up","outputId":"9d71c15d-6b66-4dd7-93bf-7582807697d6"},"source":["#Descriptive Statistics Generator for Exclusions\n","#Initial Cohort\n","initial_cohort_nrows = df_cohort_no_exclusion.count()\n","initial_cohort_npats = df_cohort_no_exclusion.select(\"subject_id\").distinct().count()\n","print((initial_cohort_nrows,initial_cohort_npats))\n","#Age Exclusion\n","df_cohort_exclusion_age = df_cohort_no_exclusion.filter((df_cohort_no_exclusion.exclusion_age == 0))\n","age_exclusion_nrows = df_cohort_exclusion_age.count()\n","age_exclusion_npats = df_cohort_exclusion_age.select(\"subject_id\").distinct().count()\n","print((age_exclusion_nrows, age_exclusion_npats))\n","#DB Exclusion\n","df_cohort_exclusion_db = df_cohort_exclusion_age.filter((df_cohort_exclusion_age.exclusion_icu_db == 0))\n","db_exclusion_nrows = df_cohort_exclusion_db.count()\n","db_exclusion_npats = df_cohort_exclusion_db.select(\"subject_id\").distinct().count()\n","print((db_exclusion_nrows, db_exclusion_npats))\n","#First Stay Exclusion\n","df_cohort_exclusion_fs = df_cohort_exclusion_db.filter((df_cohort_exclusion_db.exclusion_first_stay == 0))\n","fs_exclusion_nrows = df_cohort_exclusion_fs.count()\n","fs_exclusion_npats = df_cohort_exclusion_fs.select(\"subject_id\").distinct().count()\n","print((fs_exclusion_nrows, fs_exclusion_npats))\n","#Bad Data Exclusion\n","df_cohort_exclusion_bd = df_cohort_exclusion_fs.filter((df_cohort_exclusion_fs.exclusion_bad_data == 0))\n","bd_exclusion_nrows = df_cohort_exclusion_bd.count()\n","bd_exclusion_npats = df_cohort_exclusion_bd.select(\"subject_id\").distinct().count()\n","print((bd_exclusion_nrows, bd_exclusion_npats))\n","#Surgical Data Exclusion\n","df_cohort_exclusion_s = df_cohort_exclusion_bd.filter((df_cohort_exclusion_bd.exclusion_surgical == 0))\n","s_exclusion_nrows = df_cohort_exclusion_s.count()\n","s_exclusion_npats = df_cohort_exclusion_s.select(\"subject_id\").distinct().count()\n","print((s_exclusion_nrows, s_exclusion_npats))\n","#Sus Infection Window Data Exclusion\n","df_cohort_exclusion_sus = df_cohort_exclusion_s.filter((df_cohort_exclusion_s.exclusion_sus_inf_window == 0))\n","sus_exclusion_nrows = df_cohort_exclusion_sus.count()\n","sus_exclusion_npats = df_cohort_exclusion_sus.select(\"subject_id\").distinct().count()\n","print((sus_exclusion_nrows, sus_exclusion_npats))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(61535, 46476)\n","(53426, 38597)\n","(23617, 17707)\n","(16081, 16081)\n","(16058, 16058)\n","(13111, 13111)\n"],"name":"stdout"},{"output_type":"error","ename":"AnalysisException","evalue":"\"Reference 'exclusion_sus_inf_window' is ambiguous, could be: exclusion_sus_inf_window, exclusion_sus_inf_window.;\"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o872.apply.\n: org.apache.spark.sql.AnalysisException: Reference 'exclusion_sus_inf_window' is ambiguous, could be: exclusion_sus_inf_window, exclusion_sus_inf_window.;\n\tat org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:259)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveQuoted(LogicalPlan.scala:121)\n\tat org.apache.spark.sql.Dataset.resolve(Dataset.scala:221)\n\tat org.apache.spark.sql.Dataset.col(Dataset.scala:1274)\n\tat org.apache.spark.sql.Dataset.apply(Dataset.scala:1241)\n\tat sun.reflect.GeneratedMethodAccessor148.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-eb1627c2b54c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_exclusion_nrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_exclusion_npats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#Sus Infection Window Data Exclusion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mdf_cohort_exclusion_sus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_cohort_exclusion_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_cohort_exclusion_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexclusion_sus_inf_window\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0msus_exclusion_nrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_cohort_exclusion_sus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0msus_exclusion_npats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_cohort_exclusion_sus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subject_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1304\u001b[0m             raise AttributeError(\n\u001b[1;32m   1305\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0;32m-> 1306\u001b[0;31m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1307\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAnalysisException\u001b[0m: \"Reference 'exclusion_sus_inf_window' is ambiguous, could be: exclusion_sus_inf_window, exclusion_sus_inf_window.;\""]}]}]}